---
layout    : post
title     : "[ç¬”è®°] ç”Ÿæˆå¼æ¨èï¼šOpenOneRec æŠ€æœ¯æŠ¥å‘Šï¼ˆå¿«æ‰‹ï¼Œ2026ï¼‰"
date      : 2026-01-17
lastupdate: 2026-01-17
categories: ai recommendation
---

æœ¬æ–‡æ˜¯é˜…è¯»å­¦ä¹ å¿«æ‰‹çš„ [OpenOneRec Tech Report](https://arxiv.org/html/2512.24762v1) æ—¶æ•´ç†çš„ä¸€äº›ç¬”è®°ï¼Œ
å¾ˆå¤šè®­ç»ƒç›¸å…³çš„ä¿¡æ¯å·²ç»å¼€æºï¼Œè§ [github.com/Kuaishou-OneRec/OpenOneRec](https://github.com/Kuaishou-OneRec/OpenOneRec)ï¼Œ
åŒ…æ‹¬ï¼š

1. <strong><mark>æµ‹è¯„æ¡†æ¶</mark></strong> RecIF-Bench å’Œ<strong><mark>è®­ç»ƒæ•°æ®</mark></strong>ï¼š16w ç”¨æˆ·ï¼Œ96million äº¤äº’æ•°æ®
2. <strong><mark>æ•°æ®å¤„ç†ä»£ç ã€è®­ç»ƒä»£ç </mark></strong>ï¼Œç¡®ä¿å¯å¤ç°æ–‡ä¸­å†…å®¹ï¼ˆé pro ç‰ˆæœ¬ï¼‰
3. <strong><mark>è®­ç»ƒå¥½çš„æ¨¡å‹</mark></strong>ï¼š1.7Bã€8B

æ•´ä½“æ¡†æ¶ï¼š

<p align="center"><img src="/assets/img/openonerec/fig-2.png" width="90%" height="90%"></p>

è®­ç»ƒ&è¯„ä¼°ä»»åŠ¡ï¼š

<p align="center"><img src="/assets/img/openonerec/fig-4.png" width="90%" height="90%"></p>

ç›¸å…³æ–‡ç« ï¼š

* [ä» Tokenization è§†è§’çœ‹ç”Ÿæˆå¼æ¨èï¼ˆGRï¼‰è¿‘å‡ å¹´çš„å‘å±•ï¼ˆ2025ï¼‰]({% link _posts/2025-11-27-large-generative-recommendation-tokenization-perspective-notes-zh.md %})

æ°´å¹³åŠç»´æŠ¤ç²¾åŠ›æ‰€é™ï¼Œæ–‡ä¸­ä¸å…å­˜åœ¨é”™è¯¯æˆ–è¿‡æ—¶ä¹‹å¤„ï¼Œè¯·é…Œæƒ…å‚è€ƒã€‚
**<mark>ä¼ æ’­çŸ¥è¯†ï¼Œå°Šé‡åŠ³åŠ¨ï¼Œå¹´æ»¡åå…«å‘¨å²ï¼Œè½¬è½½è¯·æ³¨æ˜<a href="https://arthurchiao.art">å‡ºå¤„</a></mark>**ã€‚

----

* TOC
{:toc}

----


# 1 å¼•è¨€

## 1.1 RecIF-Benchï¼šæ¨èé¢†åŸŸçš„æŒ‡ä»¤éµå¾ª benchmark

æœ¬æ–‡æå‡ºäº† RecIF-Benchï¼šä¸€ä¸ª<strong><mark>æ¨èé¢†åŸŸ</mark></strong>çš„<strong><mark>æŒ‡ä»¤éµå¾ª</mark></strong>æµ‹è¯•åŸºå‡† (benchmark)ã€‚

* èƒ½è¯„ä¼° 8 ç§ä»»åŠ¡ç±»å‹ï¼Œä»åŸºç¡€æ¨èåˆ°å¤æ‚æ¨ç†
* åœºæ™¯åŒ…æ‹¬ï¼š<strong><mark>çŸ­è§†é¢‘ã€ç”µå•†ã€åœ¨çº¿å¹¿å‘Š</mark></strong>ï¼ˆshort-video, e-commerce, and online advertisingï¼‰

## 1.2 ç¼“è§£ SFT å¸¦æ¥çš„é€šç”¨èƒ½åŠ›é€€åŒ–

ä¸ºäº†ç¼“è§£ SFT å¸¦æ¥çš„é€šç”¨èƒ½åŠ›é€€åŒ–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ª<strong><mark>ä¸¤é˜¶æ®µå¯¹é½ç­–ç•¥</mark></strong>ï¼Œ
èƒ½åŒæ—¶<strong><mark>æ¢å¤é€šç”¨èƒ½åŠ›+æå‡å…·ä½“ä»»åŠ¡çš„å‡†ç¡®ç‡</mark></strong>ï¼š

1. <strong><mark><code>on-policy distillation</code></mark></strong>
2. <strong><mark><code>recommendation-oriented Reinforcement Learning</code></mark></strong> (Rec-RL)

## 1.3 å¼€æºæ¨¡å‹ï¼š1.7B/8B

æ¯ä¸ªå°ºå¯¸çš„æ¨¡å‹åˆåˆ†ä¸ºä¸¤ä¸ªç‰ˆæœ¬ï¼Œ

1. Standard ç‰ˆæœ¬ï¼šåŸºäºå¼€æºæ•°æ®è®­ç»ƒ
1. Pro ç‰ˆæœ¬ï¼šç”¨å¿«æ‰‹çš„ a hundred-billion-token industrial corpus å¢å¼º

# 2 åŸºç¡€

## 2.1. Items as Tokens: å•†å“çš„è¯­ä¹‰ç¼–ç 

å°† Item ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡æ€ï¼ˆa distinct modalityï¼‰ï¼Œé‡‡ç”¨ Itemic Tokens æ–¹æ¡ˆ (Luo et al., 2025; Zhou et al., 2025a)ï¼Œè§å›¾ 2ï¼Œ

<p align="center"><img src="/assets/img/openonerec/fig-2.png" width="90%" height="90%"></p>
<p align="center">
Figure 2 | OneRec æ•´ä½“æ¡†æ¶ã€‚<br />
(1) <mark>Pre-Training</mark>: é€šè¿‡ <mark><code>Itemic-Text Alignment</code></mark> å’Œ<strong><mark>æ¨èé¢†åŸŸ+é€šç”¨é¢†åŸŸæ•°æ®çš„è”åˆé¢„è®­ç»ƒ</mark></strong>ï¼Œä½¿æ¨¡å‹èƒ½<strong><mark>ç†è§£æ¨èé¢†åŸŸçš„ä¸šåŠ¡è¯­ä¹‰</mark></strong>ã€‚<br />
(2) <mark>Post-Training</mark>: é€šè¿‡ <strong><mark><code>SFT</code></mark></strong> è§£é”å¤šç§ä¸‹æ¸¸ä»»åŠ¡èƒ½åŠ›ï¼Œ
  ä»¥åŠé€šè¿‡äº¤æ›¿è¿›è¡Œ<strong><mark>é€šç”¨è’¸é¦</mark></strong>å’Œ<strong><mark>å¼ºåŒ–å­¦ä¹ </mark></strong>æ¥å¹³è¡¡æ¨¡å‹çš„<strong><mark>é€šç”¨æ¨ç†èƒ½åŠ›å’Œæ¨èèƒ½åŠ›</mark></strong>ã€‚<br />
(3) <mark>Evaluation</mark>: åŸºäº RecIF-Benchï¼Œä»¥åŠè¿™ Amazon æ•°æ®é›†ä¸ŠéªŒè¯è·¨é¢†åŸŸè½¬ç§»èƒ½åŠ›ã€‚
</p>

é‡‡ç”¨ <strong><mark><code>RQ-Kmeans</code></mark></strong> (Luo et al., 2025)ï¼Œå°† item metadata çš„è¯­ä¹‰ embedding ç¦»æ•£åŒ–ä¸º discrete codesã€‚

* å°† item semantics å‹ç¼©ä¸ºäº†<strong><mark>çŸ­çš„ã€å›ºå®šé•¿åº¦çš„åºåˆ—</mark></strong>ï¼Œåœ¨ä¿ç•™  collaborative structure çš„åŒæ—¶ä½¿å¾—é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡æ›´åŠ é«˜æ•ˆï¼›
* è¿™äº› tokens è‡ªå¸¦çš„å±‚çº§ç‰¹æ€§ï¼ˆhierarchical nature of these tokensï¼‰ç¡®ä¿äº†<strong><mark>è¯­ä¹‰ç±»ä¼¼çš„å•†å“ï¼Œå…±äº«ç›¸åŒçš„ prefixes</mark></strong>ï¼Œ
  ä½¿å¾—æ¨¡å‹èƒ½åŸºäº token ç›¸ä¼¼æ€§è½¬ç§»çŸ¥è¯†ï¼Œç±»ä¼¼äºè‡ªç„¶è¯­è¨€ tokens ä¸­çš„è¯­ä¹‰å…³ç³»ç¼–ç ã€‚

## 2.2. Recommendation as Auto-regressive Modelsï¼šç”¨è‡ªå›å½’æ¨¡å‹åšæ¨è

- <strong><mark>æ‰©å±•è¯è¡¨</mark></strong>ï¼šå°† item tokens æ·»åŠ åˆ°æ¨¡å‹åŸæœ‰çš„ vocabularyï¼š V = Vğ‘¡ğ‘’ğ‘¥ğ‘¡ âˆª Vğ‘–ğ‘¡ğ‘’ğ‘š. è¿™ç§æ–¹å¼ä½¿æˆ‘ä»¬èƒ½å°†ç”¨æˆ·çš„äº¤äº’å†å²ä½œä¸º text+item çš„ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡åºåˆ—ï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®ç»“æ„ï¼Œè·ŸåŸºåº§è¯­è¨€æ¨¡å‹è¿˜æ˜¯ä¸€è‡´çš„ã€‚
- è®­ç»ƒç›®æ ‡ï¼š<strong><mark><code>Next-Token Prediction</code></mark></strong>
- è®­ç»ƒä»»åŠ¡ï¼šranging from prediction (e.g., retrieval) to reasoning (e.g., explanation)

# 3 RecIF-Bench: æ¨èé¢†åŸŸçš„æŒ‡ä»¤éµå¾ª Benchmark

## 3.1 æ•°æ®é›†æ„å»º

### æ•°æ®é›†åˆ‡åˆ†ç­–ç•¥ï¼šæŒ‰ç”¨æˆ·ç»´åº¦ `80:20` åˆ‡åˆ†

åŸºäº<strong><mark>ç”¨æˆ·</mark></strong>ç»´åº¦åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚<strong><mark><code>20w</code></mark></strong> ç”¨æˆ·ï¼Œ<strong><mark>éšæœºæ‹†åˆ†</mark></strong>ï¼Œ

* 80% è®­ç»ƒ
* 20% æµ‹è¯•

## 3.2 è¯„ä¼°ä»»åŠ¡ï¼š4 å±‚ï¼Œä»å¯¹é½åˆ°æ¨ç†

RecIF-Bench å°† 8 ç±»ä»»åŠ¡åˆ†ä¸ºäº† 4 å±‚ã€‚

<p align="center"> Table 2 | RecIF-Bench ä»»åŠ¡æœ¯è¯­ï¼š8 ç±»ä»»åŠ¡åˆ†ä¸º 4 å±‚ï¼Œæè¿°äº†å®ƒä»¬çš„ input/output æ ¼å¼å’Œè¯„ä¼°é‡ç‚¹ã€‚</p>
<p align="center"><img src="/assets/img/openonerec/table-2.png" width="90%" height="90%"></p>

è®­ç»ƒæ•°æ®æ ·ä¾‹ï¼š

<p align="center"><img src="/assets/img/openonerec/fig-4.png" width="90%" height="90%"></p>
<p align="center"> Figure 4 | RecIF-Bench <strong><mark>ä»»åŠ¡ä¸¾ä¾‹</mark></strong>ã€‚We organize 8 tasks across 4 capability layers, specifying the instruction, context, and target.  </p>

### 3.2.1. Layer 0: è¯­ä¹‰å¯¹é½èƒ½åŠ›

è¯„ä¼°æ¨¡å‹æ˜¯å¦å·²ç»<strong><mark>æŠ¹å¹³ itemic tokens å’Œ natural language ä¹‹é—´çš„å·®å¼‚</mark></strong>ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰ä»»åŠ¡çš„åŸºç¡€ã€‚

- è®­ç»ƒä»»åŠ¡ï¼š
    * <strong><mark>ç»§ç»­é¢„è®­ç»ƒ</mark></strong>ï¼ˆCPTï¼‰ï¼š`Item æè¿° -> Item Token`
- è¯„ä¼°ä»»åŠ¡
    * <strong><mark><code>Item Understanding</code></mark></strong>ï¼š`Item Token -> Item textual metadata` (e.g., title, caption)

### 3.2.2. Layer 1: åŸºç¡€æ¨èèƒ½åŠ›

è¯„ä¼°æ¨¡å‹<strong><mark>æ•æ‰ç”¨æˆ·åå¥½çš„èƒ½åŠ›</mark></strong>ï¼Œé¢„æµ‹<strong><mark>ç”¨æˆ·-è´§å“äº¤äº’è¡Œä¸º</mark></strong>ï¼Œ

1. Short Video Recommendation.
1. Ad / Product Recommendation (Cross-Domain).
1. Label Prediction. Given the userâ€™s history H ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ and a candidate item ğ‘–, the model predicts whether the user will engage (e.g., effective view) with a binary <strong><mark><code>Yes/No</code></mark></strong> response.

### 3.2.3. Layer 2: æŒ‡ä»¤éµå¾ªèƒ½åŠ›

è¿™ä¸€å±‚è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½<strong><mark>å°†é¢„æµ‹èƒ½åŠ›é€‚åº”åˆ°è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸Š</mark></strong>ï¼Œä¹Ÿå°±æ˜¯è‡ªç„¶è¯­è¨€æ¨èä»»åŠ¡çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œè¿™æ˜¯åŸºäº LLM çš„æ¨èç³»ç»Ÿä¸ä¼ ç»Ÿæ¨èç³»ç»Ÿçš„æ ¸å¿ƒä¸åŒã€‚

1. <strong><mark>äº¤äº’å¼æ¨è</mark></strong>. Given the user portrait P and a natural language query ğ‘
    * è¾“å…¥ï¼š
        * ç”¨æˆ·ç”»åƒ P
        * è‡ªç„¶è¯­è¨€ query ğ‘ï¼ˆä¾‹å¦‚ï¼Œâ€œæ”¾æ¾çš„è§†é¢‘â€ï¼‰
    * è¾“å‡ºï¼š
        * ç”¨æˆ·å¯èƒ½ä¼šç§¯æäº’åŠ¨ï¼ˆç‚¹å‡»ã€ç‚¹èµã€æ”¶è—ç­‰ï¼‰çš„ç‰©å“
2. <strong><mark>æ¡ä»¶æ¨è</mark></strong>ï¼šæ›´ç»†ç²’åº¦çš„è¡Œä¸ºå»ºæ¨¡
    * è¾“å…¥ï¼š
        * ç”¨æˆ·å†å²è¡Œä¸º Hğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ
        * ç›®æ ‡è¡Œä¸º label ğ‘ï¼ˆä¾‹å¦‚ï¼Œç‚¹èµã€åˆ†äº«ç­‰ï¼‰
    * è¾“å‡ºï¼š
        * ç”¨æˆ·åœ¨ç»™å®šç›®æ ‡è¡Œä¸ºä¸‹ä¼šç§¯æäº’åŠ¨ï¼ˆç‚¹å‡»ã€ç‚¹èµã€æ”¶è—ç­‰ï¼‰çš„ç‰©å“

### 3.2.4. Layer 3: æ¨ç†èƒ½åŠ›ï¼ˆæ¨èç†ç”±ï¼‰

è¾“å…¥ï¼š

* ç”¨æˆ·ç”»åƒ P
* ç”¨æˆ·å†å²è¡Œä¸º Hğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ
* æ¨èç‰©å“ ğ‘ 

è¾“å‡ºï¼š<strong><mark>ä¸€æ®µè‡ªç„¶è¯­è¨€çš„æ¨èç†ç”±ï¼Œè§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™ä¸ªå•†å“</mark></strong>ã€‚

> Ground Truth for L3: Since reasoning tasks lack natural ground truth, we use Gemini-2.5-Pro with
> full metadata access to <strong><mark><code>generate high-quality reference outputs</code></mark></strong>.

## 3.3. è¯„ä¼°æŒ‡æ ‡

### æ¨èæŒ‡æ ‡ï¼š`Pass@K, Recall@K`

å¯¹æ¨èä»»åŠ¡ (Layer 1 & 2)ï¼Œä½¿ç”¨å¦‚ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼š

- Pass@1/Pass@32. Pass@K measures whether the ground truth item appears in the top-K generated candidates
- Recall@32. Recall@K measures the proportion of relevant items retrieved.

### æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡ï¼š`LLM-as-Judge`

å¯¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ (Layer 0 & 3), we employ LLM-as-Judge,
prompting an independent LLM to rate the generated text on dimensions such as accuracy and
coherence. è¯¦è§ Appendix B.1

# 4 Pre-Training

## 4.1 Item Tokenization

- ä¸‰å±‚é‡åŒ–ï¼Œæ¯å±‚çš„ codebook size of 8192
- Each item ğ‘– is thus mapped to a tuple of hierarchical codes ğ‘†ğ‘– = (ğ‘1, ğ‘2, ğ‘3), which is then flattened into a token sequence wrapped by special tokens:

```
<|item_begin|><item_a_5028><item_b_6733><item_c_2559><|item_end|>
```

### 4.1.1 Rec-domain è®­ç»ƒæ•°æ®

ä¸ºäº†å¢å¼ºæ¨¡å‹å¯¹ item çš„æ¨èèƒ½åŠ›ï¼Œå¯¹ item metadata æ•°æ®åˆ†ä¸ºäº†ä¸‰ç±»ï¼š

1. <strong><mark><code>Itemic Dense Caption Data</code></mark></strong>ï¼šåŸºç¡€çš„ç‰©å“è¯­ä¹‰æ•°æ®
    1. è®­ç»ƒä»»åŠ¡ï¼šç»™å®š itemic tokensï¼Œè®©æ¨¡å‹ç”Ÿæˆ corresponding natural-language caption
    2. åœ¨å•†å“çš„ SID å’Œæ–‡æœ¬æè¿°ä¹‹é—´å»ºç«‹è¯­ä¹‰æ˜ å°„ã€‚
2. <strong><mark><code>Sequential User Behavior Data</code></mark></strong>ï¼šåŸºç¡€æ¨èèƒ½åŠ›çš„æ ¸å¿ƒè®­ç»ƒè¯­æ–™

    1. å†…å®¹åŒ…æ‹¬ç”¨æˆ·çš„è§‚çœ‹ã€ç‚¹èµã€åˆ†äº«ç­‰è¡Œä¸ºã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨é•¿æœŸåºåˆ—ä¸­è¿›è¡Œ next-item predictionï¼Œæˆ‘ä»¬ä½¿å…¶èƒ½å¤Ÿå†…åŒ–åŸºç¡€çš„ååŒè¿‡æ»¤ä¿¡å·å’Œ temporal patternsã€‚ 
    2. è®©æ¨¡å‹å…·å¤‡æ ¹æ® historical behavioral trajectory é¢„æµ‹ç”¨æˆ· future interest çš„èƒ½åŠ›.

3. <strong><mark><code>Interleaved User Persona Grounding Data</code></mark></strong>ï¼šæ„å»ºé‡åŒ–ç©ºé—´çš„ deep semantic grounding

    1. åŸºäºç¦»æ•£çš„<strong><mark>ç‰©å“è¡¨ç¤º</mark></strong>å’Œå¼‚æ„çš„<strong><mark>ç”¨æˆ·å…ƒæ•°æ®</mark></strong>ï¼Œæ„å»ºäº†<strong><mark>å™äº‹é£æ ¼çš„ç”¨æˆ·ç”»åƒ</mark></strong> Pğ‘¢
        1. é™æ€å±æ€§ï¼ˆä¾‹å¦‚å¹´é¾„ã€æ€§åˆ«ï¼‰
        2. ä¸»åŠ¨æœç´¢è¡Œä¸ºï¼ˆä¾‹å¦‚æœ€è¿‘æœç´¢çš„ queryï¼‰
        3. äº¤äº’åºåˆ—ï¼ˆè¡¨ç¤ºä¸ºç‰©å“ tokens åºåˆ—ï¼‰
        4. æ€»ç»“çš„ç”¨æˆ·å…´è¶£ï¼ˆä¾‹å¦‚å†…å®¹åˆ›ä½œå†å²ã€å…³æ³¨çš„åˆ›ä½œè€…ç±»å‹ã€æ¶ˆè´¹åå¥½ï¼‰
    2. è¿™éƒ¨åˆ†æ•°æ®é›†ä¸¥æ ¼æŒ‰ç”¨æˆ·ç»´åº¦åˆ‡åˆ†ï¼Œé¿å…æ•°æ®æ³„éœ²ã€‚
        1. ä¸»è¦è®­ç»ƒè¯­æ–™åŒ…æ‹¬çº¦ 16w ç”¨æˆ·ã€1300w ç‰©å“æè¿°å’Œå¯¹åº”çš„äº¤äº’è¡Œä¸ºã€‚
        2. å¯¹äº OneRec-Proï¼Œæ‰©å±•åˆ°çº¦ 2000w ç”¨æˆ·å’Œ 9800w ç‰©å“æè¿°ã€‚
        3. è®­ç»ƒæ ·æœ¬è§ Appendix B.3ã€‚

### 4.1.2. General-domain è®­ç»ƒæ•°æ®

æ‹¿æ¨èé¢†åŸŸçš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒä¹‹åï¼Œ<strong><mark>åŸºåº§æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒä¼šè·ŸåŸæ¥æœ‰å¾ˆå¤§çš„åç§»</mark></strong>ï¼Œå¯¼è‡´ catastrophic forgettingã€‚

é€šè¿‡å¢åŠ é€šç”¨é¢†åŸŸçš„è®­ç»ƒæ•°æ®æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼š

- <strong><mark>å¤šè¯­è¨€</mark></strong> (including Chinese, English, and others)
- <strong><mark>å¤šé¢†åŸŸ</mark></strong>ï¼Œä¸»è¦æ˜¯ Coding, STEM(Science, Technology, Engineering, and Mathematics) and Medical.
- <strong><mark>å¼ºæ¨ç†æ•°æ®ä¼˜å…ˆ</mark></strong>ï¼šCrucially, to keep and further enhance the modelâ€™s reasoning capability, we prioritize reasoning-intensive data, including mathematical derivations, logical puzzles, and codecentric corpora.

æ•°æ®é›†ä¸‹è½½ï¼š

1. https://github.com/Kuaishou-OneRec/OpenOneRec
1. https://huggingface.co/datasets

<strong><mark>æ•°æ®å»é‡ç®—æ³•</mark></strong>ï¼šMinHash algorithm (Broder, 1997)

## 4.2. è®­ç»ƒé…æ–¹

we develop two model variants based on the scale of the training corpus:
1. OneRec trained exclusively on our publicly released dataset, encompassing 33B tokens across 41.3 million samples, thereby establishing a reproducible baseline for the community.
2. OneRec-Pro. leverages an extensive in-house corpus with broader user coverage, totaling 130B tokens and 179.1 million samples to achieve enhanced robustness.

åŸºåº§éƒ½æ˜¯ <strong><mark><code>Qwen3</code></mark></strong>ï¼Œæ•°æ®é…æ¯”å’Œ token é¢„ç®—è§ Appendix B.4

### Stage 1: Itemic-Text Alignmentï¼ˆå†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼‰

å»ºç«‹ itemic tokens and text tokens space ä¹‹é—´çš„<strong><mark>åˆæ­¥å¯¹é½</mark></strong>ã€‚

- å¯¹ Qwen3 çš„ tokenizer è¿›è¡Œæ‰©å±•ï¼Œè¿½åŠ  special item tokens
- è¿™ä¸ªé˜¶æ®µï¼Œåªæœ‰ item tokens ç›¸å…³çš„ embedding parameters æ˜¯å¯è®­ç»ƒçš„ï¼Œå…¶ä»–æ¨¡å‹å‚æ•°éƒ½å†»ç»“ã€‚

Note that in Qwen3, smaller models (e.g., 0.6B, 1.7B, 4B) employ tied embeddings where the embedding and output projection layers share parameters, while larger models (e.g., 8B and above) have independent output projection parameters. For larger models, the output projection parameters corresponding to itemic tokens are also trainable, ensuring proper alignment in the output space.

### Stage 2: Full-Parameter Co-Pretrainingï¼ˆå…¨å‚ç»§ç»­é¢„è®­ç»ƒï¼‰

<strong><mark>å…¨å‚é¢„è®­ç»ƒ</mark></strong>ï¼ˆfull-parameter pre-trainingï¼‰ï¼Œç»™æ¨¡å‹<strong><mark>æ³¨å…¥æ¨èé¢†åŸŸçš„çŸ¥è¯†</mark></strong>ã€‚

* ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨ä¿ç•™ Qwen3 åŸç”Ÿçš„ä¸–ç•ŒçŸ¥è¯†çš„åŒæ—¶ï¼Œèƒ½<strong><mark>æ•æ‰ç”¨æˆ·è¡Œä¸ºã€å•†å“è¯­è¨€å’Œç”¨æˆ·-å•†å“äº¤äº’ä¸­çš„å¤æ‚ pattern</mark></strong>ã€‚
* ä¸ºäº†é˜²æ­¢ catastrophic forgettingï¼Œè¿™ä¸ªé˜¶æ®µä¼šåŠ å…¥é€šç”¨é¢†åŸŸçš„çŸ¥è¯†æ•°æ®ã€‚

### Training Recipe

We use the AdamW optimizer with ğ›½1 = 0.9, ğ›½2 = 0.95, and weight decay of
0.1. The learning rate follows a cosine decay schedule with a linear warmup phase, where the peak
learning rate is set to 1 Ã— 10-3 for Stage 1 and 1 Ã— 10-4 for Stage 2, and the minimum learning
rate is set to 1 Ã— 10-4 and 2 Ã— 10-5. The warmup duration spans the first 10% of training steps.
To accommodate the long sequential nature of user behavior data, we set the maximum context
length to 32K tokens, enabling the model to process extended user interaction histories and complex
recommendation scenarios. This extended context window is crucial for capturing long-term user
preferences and understanding intricate patterns in sequential recommendation tasks.

# 5 Post-Training

<p align="center"><img src="/assets/img/openonerec/fig-2.png" width="90%" height="90%"></p>
<p align="center">
Figure 6 | Post-training pipeline of the OneRec series models
</p>

é¢„è®­ç»ƒä¹‹åï¼Œèƒ½ç†è§£å•†å“äº†ï¼Œä½†<strong><mark>æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€æ¨ç†èƒ½åŠ›å’Œé€šç”¨èƒ½åŠ›éƒ½æœ‰é€€åŒ–</mark></strong>ï¼Œä¹Ÿè¿˜ä¸èƒ½å¤„ç†å¤æ‚çš„æ¨èä»»åŠ¡ã€‚

é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„ post-training æ¥è§£å†³ä»¥ä¸Šé—®é¢˜:

1. Multi-task Supervised Fine-tuningï¼šé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æŒ‡ä»¤éµå¾ª
2. On-policy Distillationï¼šæ¢å¤é€šç”¨èƒ½åŠ›
3. Reinforcement Learning for Recommendationï¼šåœ¨æ¨èä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›å¼ºåŒ–

## 5.1. æ¢å¤é€šç”¨ instruct-following & thinking èƒ½åŠ›ï¼šå¤šä»»åŠ¡ SFT

è¿™ä¸ªé˜¶æ®µçš„ç›®çš„æ˜¯æ¢å¤å’Œå¢å¼ºæ¨¡å‹çš„åŸºç¡€æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨é€šç”¨é¢†åŸŸå’Œæ¨èé¢†åŸŸã€‚

è¯¦è§ Appendix B.5.

å‘ç°ï¼šé€šç”¨èƒ½åŠ›çš„æ¢å¤ä¹Ÿä¼šå¢å¼ºåé¢çš„æ¨èä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ã€‚

## 5.2. æ¢å¤é€šç”¨ reasoning èƒ½åŠ›ï¼šOn-policy Distillation

ä¸Šä¸€ä¸ªé˜¶æ®µæ¢å¤äº†æŒ‡ä»¤éµå¾ªå’Œ thinking çš„åŸºç¡€èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬æ³¨æ„åˆ°é€šç”¨é¢†åŸŸçš„ reasoning èƒ½åŠ›è¿˜æ˜¯ä¸¢å¤±äº†ä¸å°‘ï¼ˆa persistent capability gap in general-domain reasoningï¼‰ï¼Œ
å¯èƒ½æ˜¯ç”±äº distributional shift and the inherent sensitivity of RL-initialized backbonesã€‚
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”¨äºé€šç”¨ä»»åŠ¡çš„ on-policy distillation strategyã€‚

### Off-Policy vs. On-Policy Distillation

* off-policy distillationï¼šstudent model åœ¨ä¸€ä¸ª<strong><mark>é™æ€ã€é¢„å…ˆç”Ÿæˆçš„æ•°æ®é›†ä¸Šå­¦ä¹  teacher çš„åˆ†å¸ƒ</mark></strong>ï¼›
* on-policy distillation (Agarwal et al., 2024) ï¼šstudent model ç”Ÿæˆè‡ªå·±çš„è½¨è¿¹ï¼Œ<strong><mark>teacher æ¨¡å‹è¿›è¡Œè¯„ä¼°å’Œåé¦ˆ</mark></strong>ã€‚

### å®ç°

* æ•™å¸ˆæ¨¡å‹ï¼šä½¿ç”¨åŒç­‰è§„æ¨¡çš„ Qwen3 åŸå§‹æ¨¡å‹ä½œä¸º teacher
* ç”Ÿæˆäº† <strong><mark><code>200K</code></mark></strong> general-domain questions from the SFT dataset

### æ•ˆæœéªŒè¯

ä»è®ºæ–‡è¡¨ 10-11 å¯è§ï¼š
- Stage 1ï¼ˆSFTï¼‰â†’ Stage 2ï¼ˆåœ¨çº¿è’¸é¦ï¼‰ï¼šé€šç”¨èƒ½åŠ›æ˜¾è‘—æ¢å¤
  - MMLU-Proï¼š53.07% â†’ 54.54%
  - IFEVALï¼š61.74% â†’ 76.53%
- æœ‰æ•ˆè§£å†³äº†æŒ‡ä»¤æ¼‚ç§»é—®é¢˜ï¼ˆå¦‚å¿½ç•¥/no_think æ ‡ç­¾ä¹±ç”Ÿæˆ CoTï¼‰
- åœ¨æ¢å¤é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨èä»»åŠ¡æ€§èƒ½ï¼ˆè§è¡¨ 12ï¼‰

## 5.3. é’ˆå¯¹æ¨èä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ ï¼šGRPO

* On-policy distillation æ¢å¤äº†æ¨¡å‹çš„é€šç”¨ reasoning èƒ½åŠ›ï¼Œ
 ä½†å®ƒ<strong><mark>æ²¡æœ‰ç›´æ¥ä¼˜åŒ–æ’åºæŒ‡æ ‡</mark></strong> (e.g., Recall or NDCG)ï¼Œ
 åè€…å®šä¹‰çš„æ˜¯<strong><mark>æ¨èè´¨é‡</mark></strong>ã€‚
* SFT ä¸»è¦å…³æ³¨<strong><mark>æœ€å¤§åŒ–äº‹å®åºåˆ—çš„æ¦‚ç‡</mark></strong>ï¼ˆthe likelihood of ground-truth sequencesï¼‰ï¼Œ
  ç»å¸¸ä¼šé‡åˆ°<strong><mark>æ›å…‰åå·®</mark></strong>ï¼ˆexposure biasï¼‰é—®é¢˜ï¼Œæ— æ³•åŒºåˆ† "near-misses" and irrelevant recommendationsã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº† Recommendationoriented Reinforcement Learning (Rec-RL).

- ä½¿ç”¨ Group Relative Policy Optimization (GRPO) Shao et al. (2024) .
- traditional Actor-Critic algorithms (e.g., PPO) éœ€è¦ä¸€ä¸ªç‹¬ç«‹çš„ critic model æ¥ estimate state values, GRPO computes the advantage of a response relative to a group of sampled trajectories for the same prompt. æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒç¨³å®šæ€§ã€‚

<strong><mark><code>Rule-based Recommendation Reward</code></mark></strong>.
ä¸ºäº†å°†æ¨¡å‹å’Œ ranking accuracy å¯¹é½ï¼Œè®¾è®¡äº†ä¸€ä¸ªç¨€ç–çš„ã€åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œå…³æ³¨åœ¨"Hit" events.ã€‚

# 6 è¯„ä¼°

# 7 ç»“è®ºã€å±€é™æ€§å’Œæœªæ¥æ–¹å‘

## 7.1 Tokenizer çš„å¯è¿ç§»æ€§

å°½ç®¡æˆ‘ä»¬çš„å®éªŒè¯å®äº†ä¸€ä¸ªä¸é”™çš„åŸºåº§æ¨èæ¨¡å‹èƒ½æ˜¾è‘—æå‡ä¸‹æ¸¸æ€§èƒ½ï¼Œä½†è¿™äº›å¢ç›Šçš„å¹…åº¦ç›®å‰ä»å—é™äº <strong><mark>tokenizer çš„å¯è¿ç§»æ€§</mark></strong>ã€‚

A promising avenue for future work lies in maximizing the reuse of foundation model
priors while simultaneously ensuring high-quality item indexing (code quality) for downstream tasks.

## 7.2 æœ€ä¼˜æ•°æ®é…æ¯”

ç»´æŒæ¨¡å‹çš„<strong><mark>é€šç”¨æ™ºèƒ½ä¸æ¨ç†èƒ½åŠ›</mark></strong>éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ··åˆå¤§é‡<strong><mark>é€šç”¨é¢†åŸŸæ–‡æœ¬</mark></strong>ã€‚
ç ”ç©¶<strong><mark>æœ€ä¼˜çš„æ•°æ®é…æ¯”</mark></strong>å¹¶æå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œæ˜¯å¹³è¡¡é¢†åŸŸç‰¹å®šç²¾åº¦ä¸é€šç”¨èƒ½åŠ›çš„è¿«åˆ‡æŒ‘æˆ˜ã€‚

## 7.3 æ€ç»´é“¾æ¨ç†ç›®å‰ä»…åœ¨æœ‰é™åœºæ™¯ä¸­å¸¦æ¥æ”¹è¿›

æˆ‘ä»¬è§‚å¯Ÿåˆ°<strong><mark>æ€ç»´é“¾æ¨ç†ç›®å‰ä»…åœ¨æœ‰é™åœºæ™¯ä¸­å¸¦æ¥æ”¹è¿›</mark></strong>ã€‚
è¿™å‡¸æ˜¾äº†å¯¹ test-time scaling ç­–ç•¥è¿›è¡Œæ›´ä¸¥æ ¼æ¢ç´¢çš„å¿…è¦æ€§ï¼Œä»¥åœ¨å¤šæ ·åŒ–çš„æ¨èåœºæ™¯ä¸­å®ç°ä¸€è‡´çš„æ¨ç†å¢ç›Šã€‚

# é™„å½• B

## B.3. Pre-training æ•°æ® sampleï¼ˆæ¨èé¢†åŸŸï¼‰

### B.3.1. ç‰©å“æè¿°æ•°æ®ï¼ˆItemic Dense Caption Dataï¼‰

```
è§†é¢‘<|item_begin|><item_a_5028><item_b_6733><item_c_2559><|item_end|> å±•ç¤ºäº†ä»¥ä¸‹å†…å®¹ï¼šè§†é¢‘å†…å®¹èšç„¦åœ¨åº†ç¥å†¬è‡³è¿™ä¸€é‡è¦èŠ‚æ—¥çš„ä¹ ä¿—ï¼Œç‰¹åˆ«æ˜¯äº«å—é¥ºå­ä¸æ±¤åœ†ç­‰ç¾é£Ÿã€‚
è§†é¢‘è¡¨è¾¾äº†å†¬è‡³èŠ‚æ°”çš„ç‰¹è‰²æ„ä¹‰ï¼Œä»¥åŠäººä»¬å¯¹æ–°ä¸€å¹´å¼€å§‹çš„å¯“æ„ã€‚å†…å®¹ä¸Šï¼Œæ˜¾ç°å‡ºæµ“æµ“çš„èŠ‚æ—¥æ°”æ°›ä¸å®¶åº­æ¸©æš–ï¼Œå¯èƒ½ä¼šè§¦åŠ¨é‚£äº›å¯»æ±‚ä¼ ç»ŸèŠ‚æ—¥ä½“éªŒå’Œå®¶çš„æ„Ÿè§‰çš„è§‚ä¼—ã€‚
è§†é¢‘è¿˜å¯èƒ½æ¿€å‘è§‚ä¼—å¯¹ä¸­åä¼ ç»Ÿæ–‡åŒ–çš„å…´è¶£ï¼Œä»¥åŠå¯¹å®¶äººå›¢èšæ—¶çš„ç¾å¥½è®°å¿†ã€‚é€šè¿‡ç¾é£Ÿä¸èŠ‚æ—¥çš„ç»“åˆï¼Œè§‚ä¼—å¯æ„Ÿå—åˆ°æ¸©é¦¨å’Œå¹¸ç¦ï¼Œä¸ºå†¬è‡³èŠ‚æ—¥çš„åˆ°æ¥è¥é€ äº†æ¬¢ä¹ä¸æœŸç›¼ã€‚
```

### B.3.2. é¡ºåºç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆSequential User Behavior Dataï¼‰

```
ç”¨æˆ·çš„æ›å…‰åºåˆ—ä¸º<|item_begin|><s_a_1023><s_b_5426><s_c_6422><|item_end|>, <|item_begin|><s_a_3168><s_b_7950><s_c_4134><|item_end|>,......ï¼›
å…¶ä¸­é•¿æ’­åˆ—è¡¨æ˜¯<|item_begin|><s_a_4988><s_b_7436><s_c_2477><|item_end|>, <|item_begin|><s_a_5087><s_b_7888><s_c_4759><|item_end|>,......ï¼›
ç‚¹èµåˆ—è¡¨æ˜¯<|item_begin|><s_a_3168><s_b_7950><s_c_4134><|item_end|>, <|item_begin|><s_a_250><s_b_2310><s_c_4925><|item_end|>,......
```

### B.3.3. ç”¨æˆ·ç»¼åˆäº‹å®æ•°æ®ï¼ˆç‚¹èµã€æ”¶è—ã€è¯„è®º ... ï¼‰

Interleaved User Persona Grounding Data

```
å¹³å°ä¸Šæœ‰ä¸€åç”¨æˆ·ï¼Œå¥¹åˆ›ä½œå†…å®¹æ¶µç›–ï¼š8 ä¸ªå…¶ä»–ï¼Œ1 ä¸ªç¾é£Ÿï¼Œ1 ä¸ªæ•°ç ï¼Œ1 ä¸ªæ˜æ˜Ÿå¨±ä¹ã€‚
å¥¹è¿‘æœŸçš„æœç´¢è®°å½•åŒ…æ‹¬ï¼šæ€ä¹ˆæ‹æ¸¸æˆè§†é¢‘ã€é»‘ç™½å¤´åƒå¯çˆ±ã€......ã€‚
å¥¹è¿‘æœŸçš„è´­ä¹°è®°å½•åŒ…æ‹¬ï¼šå•†å“<|item_begin|><item_a_6133><item_b_5060><item_c_5431><|item_end|>ï¼Œå…·ä½“ç±»å‹ä¸ºã€å¥³è£…-è£¤å­-ä¼‘é—²è£¤ã€‘ï¼ŒèŠ±è´¹ 290 å…ƒã€‚
å¥¹è¿‘æœŸåœ¨è§†é¢‘<|item_begin|><item_a_3316><item_b_7440><item_c_2022><|item_end|>ä¸‹è¯„è®ºäº†"è¿™ä¸ªçŸ­å‰§å«ä»€ä¹ˆåå­—å•Š"ï¼›
åœ¨è§†é¢‘<|item_begin|><item_a_7822><item_b_1648><item_c_5756><|item_end|>ä¸‹è¯„è®ºäº†"å˜»å˜»å˜»ï¼ŒçœŸçš„å—ï¼Ÿæˆ‘ä¹Ÿå–œæ¬¢ç©è›‹ä»”æ´¾å¯¹ï¼Œæ—©å°±å…³æ³¨ä½ äº†"ï¼›......ã€‚
å¥¹ç‚¹èµäº†è§†é¢‘<|item_begin|><item_a_5743><item_b_930><item_c_1231><|item_end|>......ï¼›
æ”¶è—äº†è§†é¢‘<|item_begin|><item_a_468><item_b_8186><item_c_5877><|item_end|>......ï¼›
åˆ†äº«äº†è§†é¢‘......ã€‚å¥¹å…³æ³¨çš„åšä¸»ç±»å‹æœ‰ï¼šã€å…¶ä»–ã€‘å  47.58%ï¼Œã€é¢œå€¼ã€‘å  16.52%ï¼Œã€æ˜æ˜Ÿå¨±ä¹ã€‘å  8.37%ï¼Œ......ã€‚
å¥¹è¿‘æœŸè§‚çœ‹çš„ç›´æ’­ç±»å‹åŒ…æ‹¬ï¼šã€é—²èŠäº’åŠ¨-çƒ­é—¹é—²èŠã€‘åˆ†ç±»ä¸‹çš„ç›´æ’­ç‚¹èµäº† 6 æ¬¡ï¼Œè¯„è®ºäº† 59 æ¬¡ï¼›......
å¥¹è¿‡å» 30 å¤©è§‚çœ‹æ—¶é—´æœ€é•¿çš„ 1 ç§çŸ­å‰§ç±»å‹åˆ†åˆ«æ˜¯:[è§£å¯†_æ‚¬ç–‘]çœ‹äº† 30.0 åˆ†é’Ÿ
```

## B.4. Pre-training æ•°æ®é…æ¯”å’Œ Token Budgets

Table 13 | Data mixture for Pre-training. The table presents the distribution across general domains
and recommendation domains, showing the sampling weight of each dataset and the subtotal ratio
for each category.
<p align="center"><img src="/assets/img/openonerec/table-13.png" width="90%" height="90%"></p>

Table 14 | Data Composition and Token Budgets for Pre-training Stages. This table illustrates the
training configurations for the Open and Pro model variants across different stages, specifying the
parameter focus, data domain distribution, and allocated token budgets.
<p align="center"><img src="/assets/img/openonerec/table-14.png" width="90%" height="90%"></p>

## B.5. SFT æ•°æ®é…æ¯”å’Œ Token Budgets

Table 15 | Data Mixture for Multi-task SFT. The table presents the distribution across reasoning and
recommendation domains, showing the sampling weight of each dataset and the subtotal ratio for
each category.
<p align="center"><img src="/assets/img/openonerec/table-15.png" width="80%" height="80%"></p>


----

<a href="https://notbyai.fyi"><img src="/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg" alt="Written by Human, Not by AI"></a>
<a href="https://notbyai.fyi"><img src="/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg" alt="Written by Human, Not by AI"></a>
